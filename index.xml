<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Eddy&#39;s Boke</title>
    <link>https://Eddy-622.github.io/</link>
    <description>Recent content on Eddy&#39;s Boke</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-CN</language>
    <copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright>
    <lastBuildDate>Fri, 30 Oct 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://Eddy-622.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>数据分析之Numpy基础使用</title>
      <link>https://Eddy-622.github.io/posts/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B9%8Bnumpy%E5%9F%BA%E7%A1%80%E4%BD%BF%E7%94%A8/</link>
      <pubDate>Thu, 26 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>https://Eddy-622.github.io/posts/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B9%8Bnumpy%E5%9F%BA%E7%A1%80%E4%BD%BF%E7%94%A8/</guid>
      <description>今日内容概要  常用方法 常用方式 索引与切片 运算符 函数 随机数模块 赌场输赢案例(输赢概率五五开)  今日内容详细 补充 1.我们尽量做到将所有导入模块的语句写在文件的最上方(开头)
ps:我们可以先直接书写代码，写完之后将所有导入模块的语句全部整理到文件开头
2.在notebook环境下左侧In里面 如果是*号表示正在运行 如果是数字表示已经运行完毕
3.在执行完单元格之后 1.如果没有任何的返回结果 说明改变的是源数据 2.如果有返回结果 说明没有改变源数据,而是产生了新的结果
4.数组的特点 1.数组内所有的元素肯定是相同的数据类型 2.数组的大小无法修改 3.数组的数学运算默认都是挨个元素挨个元素逐一运算
a = array([1,2,3])a + 1 array([1+1,2+1,3+1])​ 4.数组之间做运算一定要确保彼此之间列相同
​ 5.如何查看方法的内部注释 ​
1.shift(先按) + tab(再多按几下)2.在方法的后面加?执行6.其实我们的日常生活中包括将来我们工作之后 操作浮点型的概率都是较高的 所以numpy里面默认也都是浮点型
常用方法 T 数组的转置（对高维数组而言）
li1 = [[1,2,3],[4,5,6]] a = np.array(li1) 二维数组(两行三列) a.T 转置(三行两列)dtype 数组元素的数据类型
a.dtypesize 数组元素的个数（常用）
a.sizendim 数组的维数
ar1 = np.</description>
    </item>
    
    <item>
      <title>数据分析之环境配置</title>
      <link>https://Eddy-622.github.io/posts/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B9%8B%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/</link>
      <pubDate>Wed, 25 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>https://Eddy-622.github.io/posts/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B9%8B%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/</guid>
      <description>今日内容概要  notebook的基本使用 anaconda软件下载 anaconda功能介绍 notebook快捷键操作 数据分析三大模块学习  今日内容详细 notebook的基本使用 下拉框点击python3会创建一个notebook文件
  该文件的特点 1.开发环境是基于ipython模块 2.该文件具有非常易识别的后缀名.ipynb
​ 也就意味着以后遇到该文件只能用notebook环境打开 ​ 如果你收到了后缀名是ipynb的文件应该如何正确的打开呢？ ​ 点击右上方upload按钮，将ipynb文件上传到jupyter内 ​ 即可使用notebook正常打开了
  anaconda软件介绍   虽然我们的jupyter模块已经能够给我们提供python数据分析的notebook环境，但是在数据分析过程中需要使用到大量的第三方模块(200多个)，如果仅仅基于jupyter模块那么需要我们手动下载的模块太多了
  anaconda软件内部也是提供了jupyter模块，不同的是它自动帮你下载了跟数据分析相关的200多个包(也就意味着你自己不需要下载直接可以使用) 并且anaconda内部还提供了其他的数据分析环境和一系列学习文档
  anaconda软件下载  如果你想让anaconda能够正常的呼起你的默认浏览器 一定要确保你的计算机名称中不能含有中文  下载地址:https://www.anaconda.com/products/individual download-section点击下载选择对应的系统，下载的文件是一个.exe结尾的启动文件下载完成后直接双击启动即可(按照提示一步步的点击next即可) 注意该软件下载完毕之后是不能在桌面上展示出来的，第一次启动需要你打开搜索输入anaconda 点击弹出来的anaconda navigator 等待启动完成之后右键任务栏图标把它固定到开始屏幕或者任务栏中 之后就可以鼠标双击启动  anaconda软件介绍  home主页 anaconda给我们提供的很多数据分析相关的开发环境 显示为launch的就是直接可以启动运行使用的 显示为install就是需要额外下载才可以使用的 但是我们目前只需要使用jupyter即可 Envirnments环境 anaconda提前给你准备的数据分析相关的第三方模块(200多个) learning学习环境 给你提供了很多官方权威的学习文档(有空的时候多看看) community社区环境 学习交流平台  主要功能学习 专业名词介绍
cells 单元格(当前正在被编辑的代码区)copy	拷贝paste	粘贴above	在.</description>
    </item>
    
    <item>
      <title>爬虫实例、爬取思路、数据分析简介</title>
      <link>https://Eddy-622.github.io/posts/%E7%88%AC%E8%99%AB%E5%AE%9E%E4%BE%8B%E7%88%AC%E5%8F%96%E6%80%9D%E8%B7%AF%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E7%AE%80%E4%BB%8B/</link>
      <pubDate>Tue, 24 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>https://Eddy-622.github.io/posts/%E7%88%AC%E8%99%AB%E5%AE%9E%E4%BE%8B%E7%88%AC%E5%8F%96%E6%80%9D%E8%B7%AF%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E7%AE%80%E4%BB%8B/</guid>
      <description>今日内容概要  知乎登录破解思路 红薯网小说爬取思路 爬虫框架之Scrapy安装 ipython模块 jupyter模块 anaconda数据分析软件  今日内容详细 知乎登录破解思路   网站手段 我们惯有的思路是登录成功之后网站返回给我们的cookie就是身份标识 但是有一些网站并不是这么做的 当你第一次不登陆情况下访问网站的时候，就已经偷偷给了一个cookie但是这个cookie是没有激活的状态 当你登录之后网站又会在之前的基础之上再给你一个cookie，我们会习惯性的人为第二次给的cookie才是有效的，其实不然 其实第二次给你的是个烟雾弹，真正起作用的恰巧是第一个(登录成功之后会将第一个给你的cookie激活)
1.访问登录首页查看cookie数据
 _xsrf	90d14e60-77f3-43b1-9ad0-f836a97cc631_zap	2afce640-f9c2-48dd-973a-e92c235f36382.点击用户名密码登录
有时候需要输入图片验证码有时候不需要这个其实就是由内部的https://www.zhihu.com/api/v3/oauth/captcha?lang=cn3.验证码中文英文
 lang=cn 中文汉字lang=en	英文数字4.提交post请求数据全部是加密过的 需要去js文件中查找到加密的算法 每个文件里面搜索关键字encrypt 查找到加密之前的字符串但是里面还是有url加密 需要使用python的模块继续解密
 from urllib.parse import unquote_plusclient_id 用户id(固定值)grant_type 验证方式(固定值)timestamp 时间戳*1000,去尾source	(固定值)signature	签名(js加密,变动)username	用户名password	密码captcha	验证码lang	验证码方式(固定值)utm_source	(固定值)ref_source	(固定值)other_https://www.zhihu.com/signin?next=%2F5.signature也是js加密的需要破解
 hmac加密sha1加密将上述推导步骤 熟悉一下即可</description>
    </item>
    
    <item>
      <title>爬虫之selenium模块、实例</title>
      <link>https://Eddy-622.github.io/posts/%E7%88%AC%E8%99%AB%E4%B9%8Bselenium%E6%A8%A1%E5%9D%97%E5%AE%9E%E4%BE%8B/</link>
      <pubDate>Mon, 23 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>https://Eddy-622.github.io/posts/%E7%88%AC%E8%99%AB%E4%B9%8Bselenium%E6%A8%A1%E5%9D%97%E5%AE%9E%E4%BE%8B/</guid>
      <description>今日内容概要  selenium模块 爬取京东商品信息 研究知乎防爬策略 研究小说爬取思路 Scrapy框架了解 MongoDB数据库(非关系型数据库)  今日内容详细 selenium模块   通过id查找标签
from selenium import webdriverimport timebro=webdriver.Chrome()bro.get(&amp;#34;http://www.baidu.com&amp;#34;)bro.implicitly_wait(10)inEle = bro.find_element_by_id(&amp;#39;kw&amp;#39;)往标签内写内容
inEle.send_keys(&amp;#39;美女&amp;#39;)通过a标签内部的文本查找a标签
aEle = bro.find_element_by_link_text(&amp;#39;登录&amp;#39;)点击a标签
aEle.click()如果你不知道具体文本内容 或者想模糊查询
find_element_by_partial_link_text查找div标签
divEle = bro.find_element_by_tag_name(&amp;#39;div&amp;#39;)获取内部所有的文本信息
print(divEle.text) 查看含有class属性值:
s-top-right-text c-font-normal c-color-tspEle = bro.find_element_by_class_name(&amp;#39;s-top-right-text&amp;#39;)print(spEle.text)css选择器
pEle = bro.find_element_by_css_selector(&amp;#39;#kw&amp;#39;)pEle.click()  自动登录 from selenium import webdriverimport timebro=webdriver.Chrome()bro.get(&amp;#34;http://www.baidu.com&amp;#34;)bro.implicitly_wait(10)  找到登录按钮</description>
    </item>
    
    <item>
      <title>爬虫之selenium模块</title>
      <link>https://Eddy-622.github.io/posts/%E7%88%AC%E8%99%AB%E4%B9%8Bselenium%E6%A8%A1%E5%9D%97/</link>
      <pubDate>Fri, 20 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>https://Eddy-622.github.io/posts/%E7%88%AC%E8%99%AB%E4%B9%8Bselenium%E6%A8%A1%E5%9D%97/</guid>
      <description>今日内容概要  爬取天气信息 爬取汽车之家信息 selenium模块  今日内容详细   爬取天气信息 http://tianqi.2345.com有一些网站的数据是后续js动态加载的 (网站的地址不变数据在变) 打开浏览器network点击获取数据的按钮查看内部js请求 多请求几次总结url变化规律
http://tianqi.2345.com/Pc/GetHistoryareaInfo%5BareaId%5D=60010&amp;amp;areaInfo%5BareaType%5D=2&amp;amp;date%5Byear%5D=2020&amp;amp;date%5Bmonth%5D=9http://tianqi.2345.com/Pc/GetHistoryareaInfo%5BareaId%5D=71447&amp;amp;areaInfo%5BareaType%5D=2&amp;amp;date%5Byear%5D=2020&amp;amp;date%5Bmonth%5D=10o%5BareaId%5D=60010&amp;amp;areaInfo%5BareaType%5D=2&amp;amp;date%5Byear%5D=2020&amp;amp;date%5Bmonth%5D=10 朝上述url发送get请求
  import requests import json import pandas res = requests.get(&amp;lsquo;http://tianqi.2345.com/Pc/GetHistory?areaInfo%5BareaId%5D=71447&amp;amp;areaInfo%5BareaType%5D=2&amp;amp;date%5Byear%5D=2020&amp;amp;date%5Bmonth%5D=10&#39;) content = res.content
先反序列化成python里面的字典```pythonjson_dict = json.loads(content)​	loads方法既可以反序列化json格式字符串 ​	也可以发序列化json格式的二进制数据
在利用字典取值获取天气相关的数据
real_data = json_dict.get(&amp;#39;data&amp;#39;)直接将页面上的表格数据筛选出来
res = pandas.read_html(real_data)print(res)爬取汽车之家数据 新闻标签页
 https://www.autohome.com.cn/news/总结每个页的url规律 推导出固定url格式
 https://www.autohome.com.cn/news/1/#liststarthttps://www.autohome.com.cn/news/2/#liststarthttps://www.autohome.com.cn/news/3/#liststart利用代码事先翻页的功能
url_reg = &amp;#39;https://www.autohome.com.cn/news/%s/#liststart&amp;#39;for i in range(1,6010):print(url_reg%i)尝试着直接朝url发送get请求获取数据</description>
    </item>
    
    <item>
      <title>爬虫之requests-html模块、BS4模块</title>
      <link>https://Eddy-622.github.io/posts/%E7%88%AC%E8%99%AB%E4%B9%8Brequests-html%E6%A8%A1%E5%9D%97bs4%E6%A8%A1%E5%9D%97/</link>
      <pubDate>Thu, 19 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>https://Eddy-622.github.io/posts/%E7%88%AC%E8%99%AB%E4%B9%8Brequests-html%E6%A8%A1%E5%9D%97bs4%E6%A8%A1%E5%9D%97/</guid>
      <description>今日内容概要  requests-html模块 BS4模块 爬取红牛分公司信息 爬取链家二手房信息  今日内容详细 requests-html模块 该模块的作者就是requests模块的作者
相比之下此模块更牛逼，可以直接运行js代码
  下载
pip3 install requests-html  基本使用
from requests-html import HTMLSessionr = HTMLSession.get(&amp;#39;URL&amp;#39;)a_link = r.html.linksprint(a_link)all_link = r.html.absolute_linksprint(all_link)查找标签id是downloads的标签
about = r.html.find(&amp;#39;#downloads&amp;#39;, first=True)查看该标签内部所有的文本信息
about_text = about.text查看该标签所有的属性
about_attr = about.attrs查看该标签内部包含的所有a标签
about_a = about.find(&amp;#39;a&amp;#39;)for a in about_a:print(a.text) # 获取每个a标签内部的文本 print(a.links) # 获取每个a标签内部的链接地址 requests-html官方文档
http://requests-html.kennethreitz.org/ # 习惯看英文文档   BS4模块</description>
    </item>
    
    <item>
      <title>爬虫之requests模块、cookie与session、json格式数据</title>
      <link>https://Eddy-622.github.io/posts/%E7%88%AC%E8%99%AB%E4%B9%8Brequests%E6%A8%A1%E5%9D%97cookie%E4%B8%8Esessionjson%E6%A0%BC%E5%BC%8F%E6%95%B0%E6%8D%AE/</link>
      <pubDate>Wed, 18 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>https://Eddy-622.github.io/posts/%E7%88%AC%E8%99%AB%E4%B9%8Brequests%E6%A8%A1%E5%9D%97cookie%E4%B8%8Esessionjson%E6%A0%BC%E5%BC%8F%E6%95%B0%E6%8D%AE/</guid>
      <description>今日内容概要  requests模块 cookie与session requests爬取网址数据实战演练 json格式数据  今日内容详细 requests模块 &amp;#34;&amp;#34;&amp;#34;get请求携带参数在url中携带 url？username=jason&amp;amp;hobby=readrequestes模块携带参数import requestsres = requests.get(&amp;#39;https://www.baidu.com/s&amp;#39;,params={&amp;#39;wd&amp;#39;: &amp;#39;美女&amp;#39;},headers={&amp;#39;User-Agent&amp;#39;: &amp;#39;Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/46.0.2490.76 Mobile Safari/537.36&amp;#39;,})with open(r&amp;#39;mn1.html&amp;#39;, &amp;#39;wb&amp;#39;) as f:f.write(res.content)总结:大部分网址基本上都会校验你是否是个浏览器，所以我们以后在发送请求的时候最后将User-agent携带着res = requests.get(&amp;#39;url&amp;#39;),params={字典形式携带的参数}headers={浏览器校验头，防止一般的简单网站防爬}&amp;#34;&amp;#34;&amp;#34;cookie与session &amp;#34;&amp;#34;&amp;#34;cookie与session都是用来记录当前用户状态的二者产生的原因在于HTTP协议是无状态的 &amp;gt;&amp;gt;&amp;gt; HTTP协议不记录用户信息但是网站需要记录用户的状态，登录信息等，cookie 和 session 便是分别负责在服务端与客户端进行身份识别cookie的由来就是保存在浏览器上面的k:v键值对session的由来就是保存在服务端上面的数据&amp;#39;&amp;#39;&amp;#39;在互联网中没有绝对意义上的安全&amp;#39;&amp;#39;&amp;#39;两者工作机制session的工作需要依赖于cookie总结:只要是需要保存用户状态的网址，都需要借助于cookie浏览器可以选择保存cookie也可以选择拒绝一旦浏览器拒绝保存cookie，那么所有网址的登录都无法进行知识扩展:https://www.</description>
    </item>
    
    <item>
      <title>爬虫之requests模块、正则表达式</title>
      <link>https://Eddy-622.github.io/posts/%E7%88%AC%E8%99%AB%E4%B9%8Brequests%E6%A8%A1%E5%9D%97%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/</link>
      <pubDate>Tue, 17 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>https://Eddy-622.github.io/posts/%E7%88%AC%E8%99%AB%E4%B9%8Brequests%E6%A8%A1%E5%9D%97%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/</guid>
      <description>今日内容概要   正则表达式是
用一些特殊符号的组合去字符串里筛选出符合符号特征的内容
专门在爬虫中帮助我们从网页上筛选我们想要的内容
  爬虫模块
requests模块
requets-html模块
bs4模块
  今日内容详细 正则表达式 &amp;#39;&amp;#39;&amp;#39;它是一门独立的学科，不属于任何的其他知识点所有的编程语言都可以使用正则表达式如果我们想在python中使用正则表达式需要借助于re模块(爬虫相关模块)用一些特殊符号的组合去字符串里面筛选出符合符号特征的内容&amp;#39;&amp;#39;&amp;#39;前戏 &amp;#34;&amp;#34;&amp;#34;校验手机号1.校验手机号的开头2.校验手机号的位数3.校验必须是纯数字使用python代码实现&amp;#39;&amp;#39;&amp;#39;用python逻辑代码实现&amp;#39;&amp;#39;&amp;#39;&amp;gt;&amp;gt;&amp;gt; 获取用户输入的手机号phone = input(&amp;#39;please input your number&amp;gt;&amp;gt;&amp;gt;:&amp;#39;).strip()&amp;gt;&amp;gt;&amp;gt; 1.是否是纯数字if phone.isdigit(): &amp;gt;&amp;gt;&amp;gt; 字符串里面如果是纯数字就返回True不是返回False&amp;gt;&amp;gt;&amp;gt; 2.判断位数if len(phone) == 11:&amp;gt;&amp;gt;&amp;gt; 3.判断手机号开头 11 13 14 15 18if phone.startswith(&amp;#39;11&amp;#39;) or \phone.startswith(&amp;#39;13&amp;#39;) or \phone.startswith(&amp;#39;14&amp;#39;) or \phone.startswith(&amp;#39;15&amp;#39;) or \phone.startswith(&amp;#39;18&amp;#39;):print(&amp;#39;手机号正确&amp;#39;)else:print(&amp;#39;手机号格式错误&amp;#39;)else:print(&amp;#39;手机号必须是11位&amp;#39;)else:print(&amp;#39;手机号只能是数字&amp;#39;)&amp;#39;&amp;#39;&amp;#39;用正则表达式书写&amp;#39;&amp;#39;&amp;#39;import rephone_number = input(&amp;#39;please input your phone number ： &amp;#39;)if re.</description>
    </item>
    
    <item>
      <title>初识爬虫、概念、HTML</title>
      <link>https://Eddy-622.github.io/posts/%E5%88%9D%E8%AF%86%E7%88%AC%E8%99%AB%E6%A6%82%E5%BF%B5html/</link>
      <pubDate>Mon, 16 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>https://Eddy-622.github.io/posts/%E5%88%9D%E8%AF%86%E7%88%AC%E8%99%AB%E6%A6%82%E5%BF%B5html/</guid>
      <description>今日内容概要  爬虫的概念 前端之HTML 正则表达式  今日内容详细 爬虫的概念 &amp;#39;&amp;#39;&amp;#39;互联网所有的计算机彼此互联，形成一张无形的大网联网目的实现数据的彼此传递什么是上网通过浏览器发送请求获取相应的数据（数据通过互联网传输的都是二进制格式）什么是爬虫通过代码模拟浏览器发送请求获取相应的数据兵器过滤出自己想要的存储到数据库中比喻我们把互联网当成一张很大的蜘蛛网，每台计算机上的数据便是蜘蛛的一个猎物，而爬虫程序就是一个小蜘蛛，沿着蜘蛛网抓取自己想要的猎物/数据价值 互联网最有价值的便是数据，往往谁能掌控行业的一手数据便能成为行业的主宰，这些数据代表着行业的真金白银，而爬虫就是使用工具高效的挖掘这些真金白银注意由于爬虫是有一点擦变性性质的，不要什么数据都去爬银行账户信息之类的&amp;#39;&amp;#39;&amp;#39;爬虫流程 &amp;#39;&amp;#39;&amp;#39;1. 通过代码模拟浏览器发送请求2. 必须要通过对方的校验才能获取到数据3. 解析获取到的二进制数据4、提取我们需要的保存到数据库cs架构与bs架构bs架构的本身也是cs架构通过一个浏览器访问多个服务端&amp;#39;&amp;#39;&amp;#39;HTTP协议 &amp;#39;&amp;#39;&amp;#39;HTTP协议：超文本传输协议规定了浏览器与服务端之间数据交互的格式四大特性：1、基与请求响应2、基于TCP/IP作用于应用层之上的协议3、无状态不保存用户状态4无连接（短链接）彼此交互完数据之后再无瓜葛数据格式请求格式请求首行（请求方法，HTTP协议版本）请求头（一大堆k：v键值对）请求体（post携带的请求数据会携带在请求体内）响应格式响应首行响应头响应体响应状态码用一串数字来表达信息1XX：服务端已经接受到了你的请求，正在处理你可以继续提交数据2XX：服务端应景成功的响应了对应的数据（200）3XX：重定向（原本访问A重定向到了B）4XX：404请求资源不存在，403请求不符合条件5XX：服务器内部错误（500）&amp;#39;&amp;#39;&amp;#39;请求方法 &amp;#39;&amp;#39;&amp;#39;1、get请求向别人要数据eg:1.浏览器地址栏里面输入www.baidu.com朝百度服务端要百度首页的数据2.</description>
    </item>
    
    <item>
      <title>MySQL连表查询练习题</title>
      <link>https://Eddy-622.github.io/posts/mysql%E8%BF%9E%E8%A1%A8%E6%9F%A5%E8%AF%A2%E7%BB%83%E4%B9%A0%E9%A2%98/</link>
      <pubDate>Sun, 15 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>https://Eddy-622.github.io/posts/mysql%E8%BF%9E%E8%A1%A8%E6%9F%A5%E8%AF%A2%E7%BB%83%E4%B9%A0%E9%A2%98/</guid>
      <description>作业 1、查询所有的课程的名称以及对应的任课老师姓名
SELECT course.cname,teacher.tname FROM course INNER JOIN teacher on course.teacher_id=teacher.tid;2、查询学生表中男女生各有多少人
SELECT gender as &amp;#39;性别&amp;#39;,count(gender) as &amp;#39;人数&amp;#39; FROM student GROUP BY gender ;3、查询物理成绩等于100的学生的姓名
SELECTscore.num,student.sname FROMscoreINNER JOIN student ON score.student_id = student.sid WHEREscore.course_id IN ( SELECT course.cid FROM course WHERE course.cname = &amp;#39;物理&amp;#39; ) HAVINGscore.num = 100;4、查询平均成绩大于八十分的同学的姓名和平均成绩
SELECTstudent.sname AS &amp;#39;姓名&amp;#39;,avg( score.num ) AS &amp;#39;平均成绩&amp;#39; FROMscoreRIGHT JOIN student ON score.</description>
    </item>
    
  </channel>
</rss>